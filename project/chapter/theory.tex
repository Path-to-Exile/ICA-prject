% !TEX root = ‎⁨‎⁨/../../master.tex

\section{Theory}\label{sec:theory}

In this section we outline our ICA framework and introduce the different ICA's in consideration. We consider from the start the \textit{noisy ICA} setting\footnote{See e.g. \cite{hyvarinen2001} for this nomenclature, although the assumptions on the noise are more restrictive.}. The basic structure follows \cite{hyvarinen2000}. 

	\subsection{Towards Rigidity}
	Assume we observe $n$ linear combinations of $n$ independent components with added noise variable $\bm{\epsilon}$
		\begin{align*}
			x_i=a_{i1} s_1 + a_{i2} s_2 +\ldots + a_{in} s_n+\epsilon_i, \quad \forall\, i \in \oneton.
		\end{align*} 
	Strictly speaking we should have a time index $t$ in order to differentiate between each time instant, but we have suppressed it for simplicity. 

	We consider each $x_i,s_i,\epsilon_i$ as random variables. More succinctly we can consider the random vectors for each time instant $\textbf{x}=(x_1,x_2,\ldots x_n)$, $\textbf{s}=(s_1,s_2,\ldots,s_n)$, the invertible square mixing matrix $A$ with entrances $a_{ij}$ and noise vector $\bm{\epsilon}=(\epsilon_1,\epsilon_2,\ldots,\epsilon_n)$. We can define the ICA model as
		\begin{align*}
			\textbf{x}=\textbf{A}\textbf{s}+\bm{\epsilon}.
		\end{align*} 
	We do not make any assumption on the noise term $\bm{\epsilon}$ in our initial setup as all the different algorithms under consideration impose different restrictions on it which will be made more explicit later.

	The base assumption in ICA is then that the original components are independent, i.e. $s_i\indep s_j$ for $i\neq j$. From this an unmixing matrix $A^{-1}$ which maximizes independence between the signals it recovers is to be determined in order to compute the noisy sources
		\begin{align*}
			\textbf{A}^{-1}\textbf{x}=\textbf{A}^{-1}(\textbf{A}\textbf{s}+\bm{\epsilon})=\textbf{s}+\textbf{A}^{-1}\bm{\epsilon}=\hat{\textbf{s}}.
		\end{align*} 
	One should here not be confused between \textit{sensor noise and source noise}, i.e. the difference between
		\begin{align*}
			\textbf{x}=\textbf{A}\textbf{s}+\bm{\epsilon} \quad \text{and} \quad \textbf{x}=\textbf{A}(\textbf{s}+\bm{\epsilon}).
		\end{align*} 
	The noise term in the second equation can be described as source noise where the noise is added to the independent components as a whole, which is in contrast to the first equation where the noise is added to \textit{each independent component separately}. The first equation is the noisy ICA setting which we have described, whereas the second problem simplifies to the normal ICA setting with $\hat{\textbf{s}}=s+\bm{\epsilon}$ (cf. \cite[p.94]{hyvarinen2000}).

	In the preceding paragraphs we assumed the number of observed sources was equal to the number of original sources. This assumption can be somewhat relaxed as one can also have an undercomplete ICA's, meaning there are \textit{more} observed sources than independent components. 

	The reverse problem of overcomplete ICA's where one has a higher number of original sources than observed sources is harder. We will not pursue the problem in this paper, but the reader is referred to \cite[ch.16]{hyvarinen2000}.

	\subsection{Free Appetizers}

		Before embarking on the actual ICA algorithms one often wants to do some preprocessing on the data. The two most important preprocessing steps for our present context are \textit{band-pass filtering} and \textit{whitening}.

		Band-pass filtering is done to suppress unwanted artifacts from the data. In our EEG context one may wanted to band-pass filter the data since it is known that brainwaves only exists on certain frequencies. One could therefore, in theory, filter away non-brainwaves signals.
		
		The other important preprocessing step is whitening, sometimes interchangeably referred to as PCA although PCA is in this context just a \textit{type} of whitening. The idea of whitening is to transform ones data such that all components are uncorrelated with each other and their covariance equals the identity. In the non-noisy ICA setting this process reduces the ICA search space in addition to often reducing noise and preventing over-fitting and it is therefore a standard preprocessing step in fastICA \cite[p.12]{hyvarinen2000}. However, whitening has some limitations in a noisy ICA setting.

		\subsubsection{Whitening in a Noisy World}

			We have illustrated the limitations of whitening in a noisy ICA setting in Figure \ref{fig:whitening-noisy-setting}. On the first we row in the first panel we see the original two components $s_1$ and $s_2$ which are each sampled from two i.i.d. uniform distributions. 

			The next panel show the mixed components in the non-noisy ICA setting. The components are no longer independent and they now form a parallelogram which seems to have a direction. In the last panel on the fist row we see the mixed components in the noisy ICA setting using the same mixing matrix. We have sampled the noise from a multivariate normal distribution of the form
				\begin{align*}
					\bm{\epsilon} \sim \mathcal{N}\bigg(\bigg(\begin{array}{c}  5 \\ 5 \end{array}\bigg),\bigg(\begin{array}{cc}  5 & 0 \\ 0 & 5 \end{array}\bigg)\bigg).
				\end{align*} 
			We still see the dependence in the joint distribution, but the direction is not as conspicuous as in the previous plot. 

			In the first panel on the second row we have performed whitening in the non-noisy ICA. We see whitening has completely recovered the shape of the original distribution. The only thing left is calculating the angle with which the whitened distribution has been rotated wrt. the original distribution.

			Another way to realize this is by considering the dimension of the search space. A $n$ dimensional mixing matrix has $n^2$ degrees of freedom. After whitening one only needs to estimate an orthogonal unmixing matrix which has $\nicefrac{n(n-1)}{2}$ degrees of freedom or about half as many \cite[p.160]{hyvarinen2001}. In the $2$-dimensional case this amounts to finding a single angle parameter.

			In the second panel on the second row we see whitening performed in the noisy ICA setting. We see that the original shape of the distribution has not been recovered and thus that our search space has not been reduced to just estimating an orthogonal unmixing matrix. 

			The last panel shows the same noisy ICA setting whitened, but now we have corrected for the noise. This way we have obtained the exact same rotated distribution as in the non-noisy ICA setting. This shows that whitening can be implemented meaningfully in noisy settings, but it has the very strict requirement of knowing the noise distribution. In practical settings this is rarely known and it cannot be measured as the mixed components are not observed.

			In conclusion, this example illustrates why whitening is not meaningful in an inherent noisy ICA setting. One can simply not guarantee a reduction in the search space and the step is thus unwarranted.

			\begin{figure}
				\centering
					\includegraphics[width=0.8\columnwidth]{figures/noisy-ica-example.pdf}
				\caption{Illustration of whitening in non-noisy ICA setting and why whitening can be unfruitful in a noisy ICA setting.}
				\label{fig:whitening-noisy-setting}
			\end{figure}


	\subsection{Retreating to a Safe Domain}\label{sec:ambiguities-ica}

	Assume the ICA is done correctly and the computer outputs some vector $\textbf{s}$\footnote{This is of course a hurdle in and of itself.}. The question now becomes: What does $\textbf{s}$ signify and what can be inferred from it? 

	As it turns out there are a couple ambiguities that makes a \textit{true recovery}\footnote{True is meant in the literal sense as being in (strict) accordance with reality.} of the original components impossible. 

	First we cannot identify the order of the components given only the $x^{(i)}$'s, i.e. we can never know whether $\textbf{A}^{-1}$ or $\textbf{P}\textbf{A}^{-1}$, where $\textbf{P}$ is some permutation matrix, is the true unmixing matrix since both $A$ and $s$ are unknown. 

	Further we cannot identify the true scaling of the original components either. For example consider the scaling of the mixing matrix $\nicefrac{1}{\alpha} A$, where $\alpha \in \real\setminus \{0\}$. If we scaled the original components accordingly as $\alpha\cdot s_i$ we would obtain the same $x^{(i)}$'s. In general we can scale each component with a non-zero scalar and obtain the same $x_i$ if we multiply each column in the mixing matrix with the inverse. This also means that the variance of each component $s_i$ is also impossible to recover as $\operatorname{var}(\alpha s_i)=\alpha^2 \operatorname{var}(s_i)\neq \operatorname{var}(s_i)$ in general. In summary, we cannot know which order the components are in or even their (specific) values. An illustration can be found in Appendix \ref{app:2x2-case}.

	Maybe surprisingly, this does not matter for the applications we are considering. First returning to the picture problem. The ambiguity of the order of components means that we cannot know which order the original pictures are in. The ambiguity of scaling means that the brightness of each picture is unidentifiable, but as we re-color the pictures after ICA nothing changes as their relative distance is preserved. Note the special case where the scaling is negative which inverts the color as seen in picture $1$ and $2$ in Figure \ref{fig:toy-example}.

	Regarding EEG signal, it could be interesting in itself to know exactly where in the brain each signal come from. This has been a research field known as \textit{Source Localization} since the early days of EEG signals in the 1950's, well before ICA became an area of research \cite{christoph2019}. This is however outside the scope of the present paper. Overall the ambiguities of the ICA are important concessions to be aware of but they will not stifle the specific questions we are asking. 

	\subsection{The Algorithms}\label{sec:ica-algorithms}

	We will in this section describe the different ICA algorithms. There are a lot of different algorithms in the ICA literature that deals with various aspects of the problem, let it be computation time or methodology for certain applications. It would therefore not be possible to compare all algorithms, the most we can hope for is to take some algorithms which can be deemed representative for the wealth of different algorithms in the literature. We will in this paper loosely follow the blueprint set out by \cite{pfister2019} and compare the 4 algorithms listed in Table \ref{tab:comparison-ica-algorithms}.

	\begin{table}%[htb!]
		\centering
		\begin{tabular}{llll}
			\hline&\textbf{Method} & \textbf{Signal type} & \textbf{Allowed noise} \\\hline
			1.&\text{fastICA} & \text{non-Gaussian} & \text{none} \\
			2.&\text{SOBI} & \text{fixed time-dependence} & \text{time-independent} \\
			3.&\text{choiICA (var)} & \text{varying variance} & \text{none} \\
			4.&\text{coroICA (var)} & \text{varying variance} & \text{group-wise stationary} \\\hline
		\end{tabular}
		\caption{The 4 different ICA algorithms we will be studying. Here including the different assumptions they impose on the signal type and the possible noise. Table aggregated from \cite[p.5]{pfister2019}}
		\label{tab:comparison-ica-algorithms}
	\end{table} 

	\subsubsection{fastICA} The fastICA algorithm was introduced in \cite{hyvarinen1999}. It tries to maximize negentropy by a transformation of the random variables. In fastICA a \textit{non-Gaussianity} condition is assumed from the onset in order to guarantee identifiability (up to scaling and permutation). 

	The intuition behind fastICA and the somewhat peculiar condition of non-Gaussianity is the Central Limit Theorem (CLT). Since the observed components are a linear mixture of the original components they should be more Gaussian than the original components themselves due to the CLT. Thus the signals could heuristically be unmixed by maximizing non-Gaussianity.

	It should be noted that fastICA can be extended to be robust towards Gaussian noise, i.e. 
		\begin{align*}
			\bm{\epsilon} \sim \mathcal{N}(0,\textbf{a}I_n), \quad \textbf{a}\in \real^n.
		\end{align*} 

	\subsubsection{SOBI} The SOBI algorithm was first proposed by \cite{belouchrani1997} and differs from the fastICA in that it uses a \textit{(weak) stationarity} assumption to approximate the unmixing matrix. More precisely it assumes the auto-covariance only depend on the time-lag $\tau$ not on the time $t$: \cite[p.143]{nordhausen2014}
		\begin{align*}
			E(s_ts'_{t+\tau})=\textbf{D}_\tau \text{ is diagonal for all $\tau=1,2,\ldots,$}. 
		\end{align*} 
	In practice all lags up to a certain point will be chosen and the algorithm will then try to jointly diagonalize the corresponding auto-covariance matrices \cite{nordhausen2014}. The SOBI algorithm guarantees identification (up to scaling and permutation) for ICA's with time-independent noise.

	\subsubsection{choiICA \& coroICA} Both the choiICA and the coroICA algorithms use a \textit{non-stationarity} assumption to approximate the unmixing matrix. Both algorithms exist in two main forms; a varying variance version (var)\footnote{Sometimes called the SD version, see e.g. \cite{nordhausen2014}.} and a time-dependence version (TD). For simplicity we focus on the varying variance version of the two algorithms in this paper. Both algorithms try to jointly diagonalize well-chosen blocks of covariances, including differences thereof in coroICA, to obtain the unmixing matrix. The main difference between the two algorithms is the allowed noise. 

	\cite{pfister2019} proposes the coroICA algorithm as the more robust algorithm as they prove identifiability of the unmixing matrix under group-wise stationary noise (cf. \cite[p.8]{pfister2019}). The result stands in contrast to the choiICA (var) algorithm which does not guarantee identifiability for any type of noise. See further \cite{choi2000a,choi2000b} and \cite{pfister2019}.

	Before leaving the discussion of different algorithms and their underlying assumptions, we feel compelled to repeat the common aphorism often attributed the the famous British statistician George Box: \textit{All models are wrong, but some are useful}\footnote{The first instance of Box expressing this sentiment can be found in \cite{box1976}.}. 

	It could be hypothesized that ICA algorithms which accommodate more complex noise structures perform better than those that do not on noisy, complex data like the EEG data we consider in this paper. It goes without saying that an awareness of the different model assumptions is important, but we believe the final benchmark should always be usefulness, meaning that more refined models not necessarily imply better models in practice. Usefulness of course needs to contextualized which is done Section \ref{sec:experimental-setup}.




