% !TEX root = ‎⁨‎⁨/../../master.tex

\section{Discussion}\label{sec:analysis}

	In this section we analyze and discuss our results. Throughout this section we write phrases like \say{\textit{ICA method $A$ performed better than ICA method $B$}}. This is slight misnomer as the ICA methods can only be evaluated as an (unsupervised) part of each pipeline. Statements like these should therefore be understood as \say{\textit{In pipeline $1$ (or across some pipelines), using ICA method $A$ resulted in better performance than using ICA method $B$}}. We prefer the former as it is less tedious to read and write. 
	
\subsection{Pipeline Discussion}

	We cannot unequivocally pronounce one best performing pipeline from our experiment. However, our analysis does suggest that pipelines with a QDA classifier performs better than pipelines with a RF classifier, although it should be mentioned that the pipeline with RF, no CAR and $\beta$-band had the highest mean accuracy score using fastICA of all pipelines. 

	\subsubsection{Underachieving SOBI} SOBI performed worse than the three other ICA algorithms for all pipelines, with the biggest difference being between fastICA and SOBI. fastICA performed better than SOBI about $76 \%$ percent of the time when comparing the two ICA's performance on the same subsets. The mean difference in accuracy was $1.7$ percentage point.

	SOBI's relatively bad performance may in part be because of the issue with the implementation mentioned earlier, but SOBI also performed worse than the other three algorithms across all training set sizes in \cite{pfister2019}. We therefore believe it is a fair assessment to say SOBI is the worst of the four ICA methods in our pipelines.

	\subsubsection{Many ICA's Lead to Rome} The three other algorithms relative performance was closer. fastICA was slightly above both choiICA and coroICA outperforming them in respectively $70\%$ and $62\%$ of the cases. choiICA and coroICA performance was very close to each other and the best performing algorithm changed between pipelines, most likely due to the relative low sample size of $20$. Although when comparing the two algorithms on a subset basis coroICA had a slight edge over choiICA as it performed better in $55\%$ of the cases with a mean accuracy difference of $0.36$ percentage points.

	\subsubsection{Knob Discussion} From the plots and accuracy values alone it is hard to decide whether or not we gained predictive power using both the $\alpha$- and $\beta$-band instead of just the $\beta$-band. But we can conclude that using both the $\alpha$- and $\beta$-band did not in general outperform only using the $\beta$-band, indicating that more features does not necessarily result in better performance.

	The picture becomes muddier when comparing the use of two bands to the use of one wider band encompassing both the $\alpha$- and $\beta$-range in Figure \ref{fig:band_comparison}. An experimental setup with one wider bandpass filter resembles the one seen in \cite{pfister2019} and multiple teams who worked on the present dataset in the BCI competition \cite{BCIdataset2008} \footnote{See \href{http://www.bbci.de/competition/iv/results/index.html}{here} for a short overview of the different pipelines (\url{http://www.bbci.de/competition/iv/results/index.html}). The pipelines are under dataset 2a}. 

	The accuracy scores of the two pipelines with the wide band were in general higher than those with the two smaller bands with an average mean difference of $0.790$ percentage points. Despite this, it could be interesting in the future to investigate whether a smaller band, which possibly blocks out more unwanted noise while retaining the most important brainwave signals, can lead to better performance. In addition we would also encourage a standardization of EEG bands, as discussed in \cite{newson2019}, in order to have a better basis for reference.

	Further we showed no link between using CAR and predictive power. The accuracy scores are very similar when comparing the same pipelines with and without the use of CAR. This falls in line with comments in \cite{yao2019} who questions the viability of average reference techniques in modern EEG data analysis. 

	The best performing pipeline when aggregating the four ICA methods was \textbf{QDA, CAR and $\bm\beta$-band} with an accuracy score of $0.313$. The next tier of pipelines (in descending order according by accuracy score) were \textbf{QDA, CAR, $\bm\alpha$-band and $\bm\beta$-band}, \textbf{QDA, no CAR, $\bm\alpha$-band and $\bm\beta$-band}, \textbf{QDA, no CAR and $\bm\beta$-band} and \textbf{RF, no CAR, $\bm\beta$-band} with accuracy scores between $0.302-0.306$. This corroborates on that the QDA classifier results in better performance than the RF classifier as the four pipelines with QDA all had higher mean accuracy scores than those with RF classifier.

	\subsubsection{The Linear Model}

	We also made a linear model with the different pipeline choices and different ICA methods as main effects. Before discussion the results, the limitations of such a model needs to be clear. \textit{Independence between observations is not fulfilled} since the individual subjects overlap between subset samples. We should therefore be vary of how much weight we put on the specific estimates and $p$-values. The most we can hope for is that our linear model can somehow illuminate our present problem. 

	Our linear model agrees on most of the trends we have already discussed. The model agrees in that SOBI is the worst ICA method, that the RF classifier is worse than the QDA and that CAR is not particularly important for high accuracy scores. Furthermore the LM suggests that one obtains better accuracy by only considering the $\beta$-band which is interesting since it would mean the information obtained from the $\alpha$-band is actually detrimental to the classifiers ability to correctly guess the given motor imagery action.

\subsection{Multi Subset Size Discussion}

	We choose to make our multi subset size analysis on the pipeline with \textbf{QDA, no CAR and $\bm\beta$-band}. From our pipeline analysis we believe that we could have used any of the QDA pipelines and obtained a very similar plot. However computational limitations forced us to only use one pipeline. From the earlier discussion we also choose not to use SOBI as it seems to perform the worst of the four algorithms. In general we can conclude that more training material results in better accuracy scores. Again fastICA performed the best, with choiICA and coroICA yielding very similar performances. 

	\subsubsection{Comparison to Pfister et al.} We have lower accuracy scores across all subset sizes when comparing our results with the similar setup found in \cite[p.42]{pfister2019} . One of the primary reasons for their superior scores may be, that they resampled by bootstrapping the training and test trials 50 times for each particular split of data to get a more stable estimate of the achieved classification accuracy. 

	Some of our obtained accuracy scores are not, or only very little, above random guess. Resampling could mitigate this as our analysis would be less prone to outliers. It would also relieve the problem of over-representation of outliers. However, due to computational constraints we were not able to perform this type of resampling. 

	Other factors which could explain the the discrepancy between accuracy scores is the use of a different classification algorithm. We used a standard of the shelves classifier, whereas they used a more sophisticated classifier (cf. \cite[p.29]{pfister2019} for details).

	Further coroICA (var) performed substantially better in \cite{pfister2019} outperforming fastICA, and the rest of the ICA methods, for the biggest subset sizes. We were not able to reproduce these results. More testing, e.g. by looking at different parameter settings or runtime, is needed to determine whether it is our results or \cite{pfister2019} results that need to be adjusted. 

\subsection{Conclusion}

	Using fastICA resulted in the best pipeline performance in both our pipeline study and multi subset size study. coroICA and choiICA performed similarly while SOBI consistently was the worst performing ICA method. We were however never able to get any pipeline configuration consistently above an accuracy percentage in the low 30's.

	We conclude from our pipeline study that the most important factors for obtaining a high accuracy in ones pipeline is choosing a suitable classifier and not using a bad ICA method, i.e. SOBI in this paper. Conversely, we can also conclude that a lot of different pipeline choices leads to very similar results. Due to the size of our study our results can only be deemed preliminary, but if our conclusions should hold in more generality it would have two positive consequences: 
		\begin{enumerate}[noitemsep]
			\item There exist no clear bias inducing hyper-parameter choices. 
			\item The apparent lack of standardization in the literature (cf. \cite{newson2019}) does not mean that the obtained results of the individual papers are misguiding. Other experts would more or less have obtained the same results despite their different choices in pipeline setup. 
		\end{enumerate}
	The multi subset size study further echoes our earlier discussion about the ICA methods only being one part of a larger pipeline with other equally important components. \cite{pfister2019} had better performance across all subset sizes despite the use of the same ICA methods, pointing to other factors, like more advanced sampling techniques and better classification algorithms, can have an even bigger role on performance than the specific ICA method used. 























